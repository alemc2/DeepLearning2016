%me=0 student solutions (ps file), me=1 - my solutions (sol file), me=2 - assignment (hw file)
\def\me{0}
\def\num{1}  %homework number
\def\due{Thursday, February 11}  %due date
\def\course{DS-GA.1008 Deep Learning} %course name, changed only once
\def\name{R2DEEP2 (Ankit Vani, Srivas Venkatesh)}   %student changes (instructor keeps!)
%
\iffalse
INSTRUCTIONS: replace # by the homework number.
(if this is not ps#.tex, use the right file name)

  Clip out the ********* INSERT HERE ********* bits below and insert
appropriate TeX code.  Once you are done with your file, run

  ``latex ps#.tex''

from a UNIX prompt.  If your LaTeX code is clean, the latex will exit
back to a prompt.  To see intermediate results, type

  ``xdvi ps#.dvi'' (from UNIX prompt)
  ``yap ps#.dvi'' (if using MikTex in Windows)

after compilation. Once you are done, run

  ``dvips ps#.dvi''

which should print your file to the nearest printer.  There will be
residual files called ps#.log, ps#.aux, and ps#.dvi.  All these can be
deleted, but do not delete ps1.tex. To generate postscript file ps#.ps,
run

  ``dvips -o ps#.ps ps#.dvi''

I assume you know how to print .ps files (``lpr -Pprinter ps#.ps'')
\fi
%
\title{Deep Learning}
\documentclass[11pt]{article}
\usepackage{amsfonts,amsmath,physics}
\usepackage{latexsym, graphicx}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{clrscode3e}
\usepackage{tikz}
\usepackage{bm}
\usetikzlibrary{trees}
\usepackage{tikz-qtree}
\setlength{\oddsidemargin}{.0in}
\setlength{\evensidemargin}{.0in}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-0.4in}
\setlength{\textheight}{8.5in}

\newcommand{\handout}[5]{
   \renewcommand{\thepage}{#1, Page \arabic{page}}
   \noindent
   \begin{center}
   \framebox{
      \vbox{
    \hbox to 5.78in { {\bf \course} \hfill #2 }
       \vspace{4mm}
       \hbox to 5.78in { {\Large \hfill #5  \hfill} }
       \vspace{2mm}
       \hbox to 5.78in { {\it #3 \hfill #4} }
      }
   }
   \end{center}
   \vspace*{4mm}
}

\newcommand{\LCA}{\mbox{\sf LCA}}

\newcommand{\rs}{\rightsquigarrow}
\newcommand{\ls}{\leftsquigarrow}

\newcounter{pppp}
\newcommand{\prob}{\arabic{pppp}}  %problem number
\newcommand{\increase}{\addtocounter{pppp}{1}}  %problem number

%first argument desription, second number of points
\newcommand{\newproblem}[1]{
\ifnum\me=0
\ifnum\prob>0 \newpage \fi
\increase
\setcounter{page}{1}
\handout{\name, Assignment \num, Section \arabic{pppp}}{\today}{Team: \name}{Due:
\due}{Solutions to Assignment \num}
\section*{Problem \prob~ - #1 \hfill}
\else
\increase
\section*{Problem \num-\prob~ - #1 \hfill}
\fi
}

%\newcommand{\newproblem}[2]{\increase
%\section*{Problem \num-\prob~(#1) \hfill {#2}}
%}

\def\squarebox#1{\hbox to #1{\hfill\vbox to #1{\vfill}}}
\def\qed{\hspace*{\fill}
        \vbox{\hrule\hbox{\vrule\squarebox{.667em}\vrule}\hrule}}
\newenvironment{solution}{\begin{trivlist}\item[]{\bf Solution:}}
                      {\qed \end{trivlist}}
\newenvironment{solsketch}{\begin{trivlist}\item[]{\bf Solution Sketch:}}
                      {\qed \end{trivlist}}
\newenvironment{code}{\begin{tabbing}
12345\=12345\=12345\=12345\=12345\=12345\=12345\=12345\= \kill }
{\end{tabbing}}

%%%%%\newcommand{\eqref}[1]{Equation~(\ref{eq:#1})}

\newcommand{\hint}[1]{({\bf Hint}: {#1})}
%Put more macros here, as needed.
\newcommand{\room}{\medskip\ni}
\newcommand{\brak}[1]{\langle #1 \rangle}
\newcommand{\bit}[1]{\{0,1\}^{#1}}
\newcommand{\zo}{\{0,1\}}
\newcommand{\C}{{\cal C}}

\newcommand{\nin}{\not\in}
\newcommand{\set}[1]{\{#1\}}
\renewcommand{\ni}{\noindent}
\renewcommand{\gets}{\leftarrow}
\renewcommand{\to}{\rightarrow}
\newcommand{\assign}{:=}
\newcommand{\cT}{\mathcal{T}}

\DeclareMathOperator*{\E}{E}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\ORA}{\vee}
\newcommand{\R}{\mathbb{R}}

%\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
%\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\inprod}{\langle}{\rangle}

\newcommand\Perm[2][n]{\prescript{#1\mkern-2.5mu}{}P_{#2}}

\newcommand{\AND}{\wedge}
\newcommand{\OR}{\vee}

\newcommand{\mexp}{\mathrm{e}}


\begin{document}


\newproblem{Backpropagation}


\begin{enumerate}

\item Warmup: Logistic regression is a pretty popular technique in machine learning to
classify data into two categories. This technique builds over linear regression by using
the same linear model but this is followed by the sigmoid function which converts
the output of the linear model to a value between 0 and 1. This value can then be
interpreted as a probability. This is usually represented as:
\begin{equation}
P(y=1|x_{in}) = x_{out} = \sigma(x_{in}) = \frac{1}{1+\mexp^{-x_{in}}}
\end{equation}
where $x_{in}$ as the name would suggest is the input scalar (which is also the output of
linear model) and $x_{out}$ is the output scalar.
\\If the error backpropagated to $x_{out}$ is $\pdv{E}{x_{out}}$
, write the expression for $\pdv{E}{x_{in}}$ in terms of $\pdv{E}{x_{out}}$.

\ifnum\me<2
\begin{solution}\\
Using the chain rule, we can write $\pdv{E}{x_{in}}$ as
\begin{align*}
\pdv{E}{x_{in}} &= \pdv{E}{x_{out}} \cdot \pdv{P(y=1 | x_{in})}{x_{in}}\\
&= \pdv{E}{x_{out}} \cdot \pdv{}{x_{in}} \frac{1}{1+\mexp^{-x_{in}}}\\
&= \pdv{E}{x_{out}} \cdot \frac{\mexp^{-x_{in}}}{(1+\mexp^{-x_{in}})^2}\\
&= \pdv{E}{x_{out}} \cdot \frac{\mexp^{x_{in}}}{(1+\mexp^{x_{in}})^2} \tag{multiplying numerator and denominator by $\mexp^{2x_{in}}$}
\end{align*}
\end{solution}
\fi



\item Multinomial logistic regression is a generalization of logistic regression into multiple
classes. The softmax expression is at the crux of this technique. After receiving $n$
unconstrained values, the softmax expression normalizes these values to $n$ values that
all sum to 1. This can then be perceived as probabilities attributed to the various
classes by a classifier. Your task here is to backpropagate error through this module.
The softmax expression which indicates the probability of the $i$-th class is as follows:
\begin{equation}
P(y=i|X_{in}) = (X_{out})_i = \frac{\mexp^{-\beta (X_{in})_i}}{\sum_k \mexp^{-\beta (X_{in})_k}}
\end{equation}
What is the expression for $\pdv{(X_{out})_i}{(X_{in})_j}$? (Hint: Answer differs when $i = j$ and $i \neq j$).
\\The variables $X_{in}$ and $X_{out}$ arenâ€™t scalars but vectors. While $X_{in}$ represents the $n$
values input to the system, $X_{out}$ represents the $n$ probabilities output from the system.
Therefore, the expression $(X_{out})_i$ represents the $i$-th element of $X_{out}$.
\ifnum\me<2
\begin{solution}\\
Here, we consider the components of $X_{in}$ to be independent of one another. Then, using the product rule of differentiation we get:
\begin{align*}
\pdv{(X_{out})_i}{(X_{in})_j} =
\begin{cases}
-\beta \frac{\mexp^{-\beta (X_{in})_i}}{\sum_k \mexp^{-\beta (X_{in})_k}}
+ \beta \left( \frac{\mexp^{-\beta (X_{in})_i}}{\sum_k \mexp^{-\beta (X_{in})_k}} \right)^2
= \beta (X_{out})_i \left((X_{out})_i -1\right)
&\text{if } i=j\\\\
\beta \frac{\mexp^{-\beta (X_{in})_i} \cdot \mexp^{-\beta (X_{in})_j}}{\left( \sum_k \mexp^{-\beta (X_{in})_k} \right)^2}
= \beta (X_{out})_i (X_{out})_j
&\text{if } i \neq j 
\end{cases}
\end{align*}
\end{solution}
\fi


\end{enumerate}



\newproblem{Torch (MNIST Handwritten Digit Recognition)}

\ifnum\me<2
\begin{solution}
\\We performed experiments on the MNIST dataset trying to classify on it using a 4 layered network. We achieved an accuracy of $xxx\%$ on the test set with our model. A detailed explanation of our experiments is mentioned below:
\begin{enumerate}
\item Overview of the network
\begin{itemize}
\item Dataset and partitioning:
\\We used the MNIST handwritten character dataset for these experiments. It consists of $60,000$ data points in the training set and another $10,000$ data points in the test set. For the purpose of our experiments we split the training set as $55,000$ points for training the network and the remaining $5,000$ points for tuning the parameters using cross-validation.
\\The data that was loaded was normalized to have zero mean and unit variance. The parameters used for this normalization were stored and used to normalize the cross-validation and test set as well.

\item Model and Loss Function:
\\The model starts with a convolution layer of $5 \times 5$ filter bank of size $32$. This is followed by a ReLu layer and a pooling layer. This is followed by another convolution layer of $5 \times 5$ filter bank of size $64$ which is followed by another set of ReLu and pooling layers. Then it is followed by a linear layer mapping to 1024 points which is terminated by a dropout layer and another linear layer resulting in 10 outputs.
\\The Loss function we used was the negative log likelihood measure.

\item Training and distortions:

\item Testing:
\end{itemize}

\item Experiments and observations:
\begin{table}[h]
\centering
\begin{tabular}{| l | l |}
\hline
\textbf{Network} & \textbf{Accuracy} \\ \hline
default & 0.980 \\ \hline
tanh to relu (r) & 0.983 \\ \hline
(r)+ dropout at input & 0.973 \\ \hline
(r)+ spatial dropout at input & 0.979 \\ \hline
(r)+ spatial dropout after first conv layer relu & 0.984 \\ \hline
(r)+ dropout before first linear layer (d) & 0.991 \\ \hline
(r)+ spatial dropout after second conv layer relu (e) & 0.992 \\ \hline
(d,e)& 0.993 \\ \hline
(d,e)+ lower spacial dropout (e) from 0.5 to 0.25 & 0.991 \\ \hline
(d,e)+ lower dropout (d) from 0.5 to 0.25 & 0.993 \\ \hline
(d,e)+ lower both dropouts from 0.5 to 0.25 & 0.993 \\ \hline
(d,e)+ increase both dropouts from 0.5 to 0.6 & 0.983 \\ \hline
(d,e)+ conv 7$\times$7, 5$\times$5, new linear layer 1024 $\to$ 512 $\to$ 128 $\to$ 10 & 0.991 \\ \hline
(d)+ random image rotations and translations & 0.993 \\ \hline
(d)+ random image rotations, translations, shearing (s) & 0.992 \\ \hline
(s)+ conv(32 $\to$ 64) $\to$ linear(1024 $\to$ 10) (f) & \textbf{0.994} \\ \hline
\end{tabular}
\caption{Validation accuracies with different network structures.}
\label{table:acc}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{| l | l | l |}
\hline
\textbf{Optimizer} & \textbf{Hyperparameters} & \textbf{Accuracy} \\ \hline
SGD & LR = $10^{-3}$, wt. decay = 0, momentum = 0, LR decay = $10^{-7}$ & 0.983 \\ \hline
ADAM & LR = $10^{-3}$, $\beta_1$ = 0.9, $\beta_2$ = 0.999, $\epsilon$ = $10^{-8}$ & \textbf{0.994} \\ \hline
ADAGRAD & LR = $10^{-3}$ & 0.989 \\ \hline
ADADELTA & & \textbf{0.994} \\ \hline
\end{tabular}
\caption{Validation accuracies for structure (f) from Table \ref{table:acc} using different optimizers.}
\end{table}

We also tried the other losses in the sample code, without getting any conclusive difference.

\item Intuition:

\item Results and Remarks:
\end{enumerate}
\end{solution}

\fi



\end{document}


